{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb195c3",
   "metadata": {},
   "source": [
    "#Coding Week ML TASK1\n",
    " \n",
    "This notebook includes all steps including EDA,cleaning,preprocessing, model training and evaluation that lies behind the solution of provided ML task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first import all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "#for scikit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893445fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstly we load dataset using pandas\n",
    "#let's name dataset df for simplicity\n",
    "df = pd.read_csv(\"tour_logs_test_input.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869ecc7",
   "metadata": {},
   "source": [
    "Now that the dataset is loaded, inspect few first rows using head(); we can even load few rows from back using tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73519ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first few rows\n",
    "df.head()\n",
    "#last few rows\n",
    "df.tail()\n",
    "#we can also load a few rowsncols using loc function\n",
    "df.loc[28:39]\n",
    "\n",
    "#now we can load only certain specific cols \n",
    "#for better exploration of data before analysisng\n",
    "df.loc[31:69,['Venue_ID','Ticket_Price','Crowd_Size']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b368121",
   "metadata": {},
   "source": [
    "Now we check basics for given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#information about df\n",
    "df.info()\n",
    "#summary of df\n",
    "df.describe()\n",
    "#size of df\n",
    "print(df.size())\n",
    "#shape of df ie rows by cols\n",
    "print(df.shape)\n",
    "#for columns\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8b0b7",
   "metadata": {},
   "source": [
    "Data preprocessing:\n",
    "handling missing values, feature scaling, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fbe7e",
   "metadata": {},
   "source": [
    "We can firstly drop col that are unnecessary in EDA such as gig_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1.drop(column=['Gig_ID'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce694fe",
   "metadata": {},
   "source": [
    "Then we find null values in remaning rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f98bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding null values\n",
    "#using this we find how many cols have null values\n",
    "print(df1.isnull().sum())\n",
    "#percentage of null rows\n",
    "print(df1.isnull().sum() * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a747a6e",
   "metadata": {},
   "source": [
    "Here  volume_level and crowd_size have 8.4% and 2.2% missing values only; which is comparitively quiet small compared to number of rows present in dataset.\n",
    "so, we can drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping null values\n",
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107fcd32",
   "metadata": {},
   "source": [
    "Identifying and handling duplicate values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b02019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying rows which are exact duplicates\n",
    "duplicate_rows = df1.duplicated()\n",
    "print('duplicated rows:', duplicate_rows.sum())\n",
    "#removing them\n",
    "df1 = df1.drop_duplicates()\n",
    "#drop_duplicates() removes exact row duplicates keeping first common occurance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa742658",
   "metadata": {},
   "source": [
    "Fixing errors in dataset\n",
    "-The dates are inconsistent but in data evaluation those won't come handy as we have already dropped earlier\n",
    "-Ticket prices are inconsistent and are to be converted in a certain single unit.\n",
    "-applying certain changes like absolute and constraints "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103acb7",
   "metadata": {},
   "source": [
    "We have prices in other units. so, we run another function to convert them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56943e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting\n",
    "def conversion_in_usd(y):\n",
    "    y = str(y).strip()\n",
    "    \n",
    "    if y.lower() == 'free':\n",
    "        return 0.0  # free ticket\n",
    "    \n",
    "    # take first meaningful part\n",
    "    a = y.split()[0]\n",
    "\n",
    "    if a.startswith('£'):\n",
    "        return float(a[1:]) * 1.27\n",
    "    elif a.startswith('€'):\n",
    "        return float(a[1:]) * 1.09\n",
    "    elif a.startswith('$'):\n",
    "        return float(a[1:])\n",
    "    elif a.endswith('USD'):\n",
    "        return float(a.replace('USD', ''))\n",
    "    else:\n",
    "        return float(a)\n",
    "df1['Ticket_Price_USD'] = df1['Ticket_Price'].apply(conversion_in_usd)\n",
    "print(df1['Ticket_Price_USD'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3480d",
   "metadata": {},
   "source": [
    "As the date & time are in inconsistent format, we run a function to make them in one consistent format YYYY-MM-DD and HH:MM:SS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c37b0",
   "metadata": {},
   "source": [
    "First we will work on date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999891a",
   "metadata": {},
   "source": [
    "We'll handle the textual ones first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1['Show_Date'].str.contains('morning', case=False, na=False), 'Time_Bucket'] = 'Morning'\n",
    "df1.loc[df1['Show_Date'].str.contains('afternoon', case=False, na=False), 'Time_Bucket'] = 'Afternoon'\n",
    "df1.loc[df1['Show_Date'].str.contains('evening', case=False, na=False), 'Time_Bucket'] = 'Evening'\n",
    "df1.loc[df1['Show_Date'].str.contains('late night', case=False, na=False), 'Time_Bucket'] = 'Late Night'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88dd875",
   "metadata": {},
   "source": [
    "for dd/mm/yyyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddmmyyyy = df1['Show_Date'].str.contains(r'\\d{2}/\\d{2}/\\d{4}', na=False)\n",
    "df1.loc[ddmmyyyy, 'Clean_Date'] = pd.to_datetime(\n",
    "    df1.loc[ddmmyyyy, 'Show_Date'],\n",
    "    dayfirst=True,\n",
    "    errors='coerce'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7de74",
   "metadata": {},
   "source": [
    "for yyyy-dd-mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_yyyyddmm = df1['Show_Date'].str.contains(r'\\d{4}-\\d{2}-\\d{2}', na=False)\n",
    "df1.loc[mask_yyyyddmm, 'Clean_Date'] = pd.to_datetime(\n",
    "    df1.loc[mask_yyyyddmm, 'Show_Date'],\n",
    "    format='%Y-%d-%m',\n",
    "    errors='coerce'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b19c8",
   "metadata": {},
   "source": [
    "for (month date, year) type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming for ease in code\n",
    "df1['Show_DateTime']=df1['datentime']\n",
    "#removing uncessary spaces\n",
    "df1['datentime']=df1['datentime'].str.strip()\n",
    "#writing function\n",
    "def parse_date(x):\n",
    "    try:\n",
    "        # If full MonthDayYear\n",
    "        return pd.to_datetime(x, format='%b%d,%Y')\n",
    "    except:\n",
    "        try:\n",
    "            # If only Day,Year → add default month (e.g., Jan)\n",
    "            return pd.to_datetime('Jan' + x, format='%b%d,%Y')\n",
    "        except:\n",
    "            try:\n",
    "                # If only MonthDay → add default year (e.g., 2024)\n",
    "                return pd.to_datetime(x + ',2024', format='%b%d,%Y')\n",
    "            except:\n",
    "                return pd.NaT\n",
    "\n",
    "df1['Clean_Date'] = df1['Show_Date'].apply(parse_date)\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31debc38",
   "metadata": {},
   "source": [
    "some rows include volume rate as negatives; so we replace such values in positives as it's likely a data entry error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Volume_Level']=df1['Volume_Level'].abs()\n",
    "print(df1['Volume_Level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1572f",
   "metadata": {},
   "source": [
    "Now that we have clean dataset; we can move forward to EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cef8c",
   "metadata": {},
   "source": [
    "EDA:\n",
    "-understanding patterns\n",
    "we can visualise parameters available like day_of_week over exploring possibility of ticket_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f043ac",
   "metadata": {},
   "source": [
    "Checking venue categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count\n",
    "print(df1['Venue_ID'].value_counts())\n",
    "print(df1.groupby('Venue_ID')['Crowd_Size'].std())\n",
    "#plot\n",
    "sns.countplot(x='Venue_ID', data=df1)\n",
    "plt.title('Number of Shows per Venue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fbf3e",
   "metadata": {},
   "source": [
    "TESTING AS PER SCRIBBLES\n",
    "-tracking for day of week ~ (tuesdays curse?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8c1b7",
   "metadata": {},
   "source": [
    " checking for other factors like volume level, weather, etc if they roleplay in affecting\n",
    " tuesday shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571530a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing tuesdays with other days\n",
    "#mean\n",
    "print(df1.groupby('Day_of_Week')['Crowd_Size'].mean())\n",
    "#variance\n",
    "print(df1.groupby('Day_of_Week')['Volume_Level'].std())\n",
    "#plot\n",
    "sns.boxplot(x='Day_of_Week', y='Opener_Rating', data=df1)\n",
    "plt.show()\n",
    "#mean\n",
    "print(df1.groupby('Day_of_Week')['Opener_Rating'].mean())\n",
    "#for tuesdays only\n",
    "tuesdays = df1[df1['Day_of_Week'] == 1]\n",
    "print(\"Tuesday shows statistics:\")\n",
    "print(tuesdays.describe())\n",
    "#plot\n",
    "sns.boxplot(x='Day_of_Week', y='Crowd_Size', data=df1)\n",
    "plt.title('Crowd Size by Day of Week')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84bd46d",
   "metadata": {},
   "source": [
    "Here tuesdays have 498.85 as mean which is less compared to comparitive average of 533.71 over week; On tuedays, we have volume level's mean = 5.48 which is comparable to avg value of 523 Here, tuesday's have variance of 3.51 which is more compared to avg of 3.33; but it doesn't have highest std so it aint that unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf98d1",
   "metadata": {},
   "source": [
    "Calculating bad days as those when volume level is greater than average levels; then compare it for days in week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f371920",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Bad_Event'] = (df1['Volume_Level'] < df1['Volume_Level'].mean()).astype(int)\n",
    "print(df1.groupby('Day_of_Week')['Bad_Event'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7172d17",
   "metadata": {},
   "source": [
    "On tuesdays, we get 0.56 around which is slight higher than avg value 0.54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1c1d5",
   "metadata": {},
   "source": [
    "-Next, \n",
    "-full moon=best shows\n",
    "-volume level\n",
    "-leather jacket-mid\n",
    "-spendex right- but data otherwise\n",
    "-rain sucks\n",
    "-v delta analysis\n",
    "-mosh pits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69872d1",
   "metadata": {},
   "source": [
    "moon-phase and show analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d31eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#moon phase x vol levels\n",
    "print(df1.groupby('Moon_Phase')['Volume_Level'].mean())\n",
    "#moon phase x crowd size\n",
    "print(df1.groupby('Moon_Phase')['Crowd_Size'].mean())\n",
    "#mean\n",
    "print(df1.groupby('Moon_Phase')['Opener_Rating'].mean())\n",
    "#count\n",
    "df1.groupby('Moon_Phase')['Opener_Rating'].count()\n",
    "#plot moon phase x vol level\n",
    "sns.boxplot(x='Moon_Phase', y='Volume_Level', data=df1)\n",
    "plt.show()\n",
    "#plot moon phase x crowd size\n",
    "sns.boxplot(x='Moon_Phase', y='Crowd_Size', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf06a00",
   "metadata": {},
   "source": [
    "full moon = 5.14; avg is 5.9 is very small-\n",
    "it's quiet less compared to avg\n",
    "least on full moon - 491.07\n",
    "From boxplot, we observe full moon has 4.92 outlier with mean 5.14 and nothing exceptional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdfa471",
   "metadata": {},
   "source": [
    "Outfit via checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#band fit x crowd size\n",
    "print(df1.groupby('Band_Outfit')['Crowd_Size'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76534836",
   "metadata": {},
   "source": [
    "Leather jacket has 512.38 mean crowd size compared to avg of 537.87; which quiet less compared to usual.\n",
    "outfit analysis - spandex feels right n has 541.53 avg;\n",
    "velvet-546.62 n highest is denim-550.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3543906",
   "metadata": {},
   "source": [
    "weather wise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f800ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.groupby('Weather')['Crowd_Size'].mean())\n",
    "print(df1.groupby('Weather')['Opener_Rating'].mean())\n",
    "#venue related\n",
    "print(df1.groupby(['Venue_ID','Weather'])['Opener_Rating'].mean())\n",
    "#plot\n",
    "sns.boxplot(x='Weather', y='Crowd_Size', data=df1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9064f7",
   "metadata": {},
   "source": [
    "obeserved from boxplot that cloudy-508, clear-529,stormy-512, rainy-532\n",
    "mean- full moon=491.07 less-least"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9f062",
   "metadata": {},
   "source": [
    "mosh pits analysis - \n",
    "-monks vs mosh pit\n",
    "\n",
    "-analysed for: volume levels, variance level, crowd size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef497abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#venue wise analysing\n",
    "df.groupby('Venue_ID')['Volume_Level'].describe()\n",
    "#mosh pits at v_delta\n",
    "delta = df1[df1['Venue'] == 'V_Delta']\n",
    "sns.scatterplot(x='Volume_Level', y='Crowd_Size', data=delta)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004ea5c",
   "metadata": {},
   "source": [
    "-pricing sensitivity across levels based on ticket_prices\n",
    "-reactions-ratings as per ticket prices groupwise analysed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5fb42",
   "metadata": {},
   "source": [
    "Creating a price bucket using certain range to test singer's claims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing certain range\n",
    "def price_bucket(p):\n",
    "    if p < 50:\n",
    "        return 'Low (<50)'\n",
    "    elif 50 <= p < 60:\n",
    "        return 'Mid (50 to 60)'\n",
    "    elif 60 <= p < 70:\n",
    "        return 'Sweet Spot (60 to 70)'\n",
    "    else:\n",
    "        return 'High (70+)'\n",
    "#\n",
    "df1['Price_Bucket'] = df1['Ticket_Price_USD'].apply(price_bucket)\n",
    "print(df1['Price_Bucket'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea232e",
   "metadata": {},
   "source": [
    "price x ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot for average rating by price bucket\n",
    "sns.barplot(x='Price_Bucket', y='Opener_Rating', data=df1, ci=None, palette='viridis')\n",
    "plt.title('Average Rating by Ticket Price Bucket')\n",
    "plt.ylabel('Average Rating (0-5)')\n",
    "plt.xlabel('Price Bucket')\n",
    "plt.xticks(rotation=15)  # rotate if needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b6683",
   "metadata": {},
   "source": [
    "price x rating x venue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlating by vol level\n",
    "print(df1[['Volume_Level','Crowd_Size','Opener_Rating']].corr())\n",
    "#corr by venue\n",
    "print(df1.groupby('Venue_ID')[['Volume_Level','Crowd_Size','Opener_Rating']].corr())\n",
    "price_order = ['Low (<50)', 'Mid (50–60)', 'Sweet Spot (60–70)', 'High (70+)']\n",
    "sns.barplot(\n",
    "    x='Price_Bucket',\n",
    "    y='Opener_Rating',\n",
    "    hue='Venue_ID',\n",
    "    data=df1,\n",
    "    ci=None,\n",
    "    order=price_order\n",
    ")\n",
    "plt.title('Ratings Across Price Buckets per Venue')\n",
    "plt.ylabel('Average Rating (0-5)')\n",
    "plt.xlabel('Price Bucket')\n",
    "plt.legend(title='Venue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd41401",
   "metadata": {},
   "source": [
    "EDA was done on tour_logs_test_inputs.csv \n",
    "now for feature engineering and modeling use tour_logs_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cedc9e4",
   "metadata": {},
   "source": [
    "making certain important data curation on train.csv too cause the dataset is unfiltered and messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"tour_logs_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ba17b",
   "metadata": {},
   "source": [
    "Feature engineering\n",
    "-Droping leakage cols\n",
    "-extracting important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d1600",
   "metadata": {},
   "source": [
    "Doing all basics for train.csv too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2)\n",
    "#information about df\n",
    "df2.info()\n",
    "#summary of df\n",
    "df2.describe()\n",
    "#size of df\n",
    "print(df2.size)\n",
    "#shape of df ie rows by cols\n",
    "print(df2.shape)\n",
    "#for columns\n",
    "print(df2.columns)\n",
    "print(df2.isnull().sum())\n",
    "#percentage of null rows\n",
    "print(df2.isnull().sum() * 100)\n",
    "df2.dropna(inplace=True)\n",
    "#identifying rows which are exact duplicates\n",
    "duplicate_rows = df2.duplicated()\n",
    "#removing \n",
    "#prices in usd\n",
    "def conversion_in_usd(y):\n",
    "    y = str(y).strip()\n",
    "    \n",
    "    if y.lower() == 'free':\n",
    "        return 0.0  # free ticket\n",
    "    \n",
    "    # take first meaningful part\n",
    "    a = y.split()[0]\n",
    "\n",
    "    if a.startswith('£'):\n",
    "        return float(a[1:]) * 1.27\n",
    "    elif a.startswith('€'):\n",
    "        return float(a[1:]) * 1.09\n",
    "    elif a.startswith('$'):\n",
    "        return float(a[1:])\n",
    "    elif a.endswith('USD'):\n",
    "        return float(a.replace('USD', ''))\n",
    "    else:\n",
    "        return float(a)\n",
    "\n",
    "df2['Ticket_Price_USD'] = df2['Ticket_Price'].apply(conversion_in_usd)\n",
    "#dates\n",
    "df2.loc[df2['Show_DateTime'].str.contains('morning', case=False, na=False), 'Time_Bucket'] = 'Morning'\n",
    "df2.loc[df2['Show_DateTime'].str.contains('afternoon', case=False, na=False), 'Time_Bucket'] = 'Afternoon'\n",
    "df2.loc[df2['Show_DateTime'].str.contains('evening', case=False, na=False), 'Time_Bucket'] = 'Evening'\n",
    "df2.loc[df2['Show_DateTime'].str.contains('late night', case=False, na=False), 'Time_Bucket'] = 'Late Night'\n",
    "#dd-mm-yyyy\n",
    "ddmmyyyy = df2['Show_DateTime'].str.contains(r'\\d{2}/\\d{2}/\\d{4}', na=False)\n",
    "df2.loc[ddmmyyyy, 'Clean_Date'] = pd.to_datetime(\n",
    "    df2.loc[ddmmyyyy, 'Show_DateTime'],\n",
    "    dayfirst=True,\n",
    "    errors='coerce'\n",
    ")\n",
    "#yyyy-dd-mm\n",
    "mask_yyyyddmm = df2['Show_DateTime'].str.contains(r'\\d{4}-\\d{2}-\\d{2}', na=False)\n",
    "df2.loc[mask_yyyyddmm, 'Clean_Date'] = pd.to_datetime(\n",
    "    df2.loc[mask_yyyyddmm, 'Show_DateTime'],\n",
    "    format='%Y-%d-%m',\n",
    "    errors='coerce'\n",
    ")\n",
    "#renaming for ease in code\n",
    "#removing uncessary spaces\n",
    "df2['Show_DateTime']=df2['Show_DateTime'].str.strip()\n",
    "#writing function\n",
    "def parse_date(x):\n",
    "    try:\n",
    "        # If full MonthDayYear\n",
    "        return pd.to_datetime(x, format='%b%d,%Y')\n",
    "    except:\n",
    "        try:\n",
    "            # If only Day,Year → add default month (e.g., Jan)\n",
    "            return pd.to_datetime('Jan' + x, format='%b%d,%Y')\n",
    "        except:\n",
    "            try:\n",
    "                # If only MonthDay → add default year (e.g., 2024)\n",
    "                return pd.to_datetime(x + ',2024', format='%b%d,%Y')\n",
    "            except:\n",
    "                return pd.NaT\n",
    "df2['Clean_Date'] = df2['Show_DateTime'].apply(parse_date)\n",
    "#correcting negative volume level\n",
    "df2['Volume_Level']=df2['Volume_Level'].abs()\n",
    "#again dropping null values if any present\n",
    "df2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6856d",
   "metadata": {},
   "source": [
    "Droping merch sale with certain cols is significant because this data is recorded post-show and ain't involved in prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195845d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "leakage_cols = [['Merch_Sales_Post_Show'],['Moon_Phase'],['Band_Outfit']]\n",
    "df2 = df2.drop(columns=leakage_cols)\n",
    "df2.shape\n",
    "print(df2.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf9b69",
   "metadata": {},
   "source": [
    "Extracting features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#price per capacity ratio\n",
    "df2['Price_per_Crowd'] = df2['Ticket_Price_USD'] / (df2['Crowd_Size'] + 1)\n",
    "#crowd density\n",
    "#using manager's venue capacity\n",
    "df2['Crowd_Density'] = df2['Crowd_Size'] / 800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weekend wise\n",
    "df2['Is_Weekend'] = df2['Day_of_Week'].isin(\n",
    "    ['Friday', 'Saturday']\n",
    ").astype(int)\n",
    "\n",
    "#making time bucket\n",
    "def time_bucket(hour):\n",
    "    if hour < 12:\n",
    "        return 'Morning'\n",
    "    elif hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Late Night'\n",
    "\n",
    "df2['Time_Bucket'] = df2['Show_DateTime'].dt.hour.apply(time_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f654a",
   "metadata": {},
   "source": [
    "Venue wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count x venue\n",
    "df2['Show_Count_Venue'] = df2.groupby('Venue_ID').cumcount()\n",
    "#vol x venue\n",
    "df2['Volume_x_Venue'] = df2['Volume_Level'] * df2['Venue_ID'].astype('category').cat.codes\n",
    "#price x venue\n",
    "df2['Price_x_Venue'] = df2['Ticket_Price'] * df2['Venue_ID'].astype('category').cat.codes\n",
    "#weather x venue\n",
    "df2['Bad_Weather'] = df2['Weather'].isin(['Rain', 'Storm']).astype(int)\n",
    "df2['Weather_x_Venue'] = df2['Bad_Weather'] * df2['Venue_ID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964312fe",
   "metadata": {},
   "source": [
    "Log transfromed skew features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddaeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Log_Crowd_Size'] = np.log1p(df2['Crowd_Size'])\n",
    "df2['Log_Ticket_Price'] = np.log1p(df2['Ticket_Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8581fa74",
   "metadata": {},
   "source": [
    "checking Stablity with certain new indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helps in mosh pits feedback\n",
    "df2['Chaos_Index'] = (\n",
    "    df2['Volume_Level'] * df2['Crowd_Density']\n",
    ")\n",
    "#pricing based\n",
    "#helps in understanding here relative > absolute pricing\n",
    "df2['Price_Shock'] = (\n",
    "    df2['Ticket_Price_USD'] > df2.groupby('Venue_ID')['Ticket_Price_USD'].transform('mean')\n",
    ").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataset concisely having new features added\n",
    "df_model = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93d93c",
   "metadata": {},
   "source": [
    "Now, we head to modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y = pd.to_numeric(df2['Crowd_Energy'], errors='coerce')\n",
    "X = df2.drop(columns=['Crowd_Energy'])\n",
    "# Clean NaNs from Target\n",
    "mask = y.notna()\n",
    "y = y[mask]\n",
    "X = X.loc[mask]\n",
    "# Clean Features:\n",
    "# Drop Date/Time columns\n",
    "# Keep only Numbers and Text\n",
    "X = X.select_dtypes(include=['number', 'object', 'category'])\n",
    "# Encode Text to Numbers\n",
    "X = pd.get_dummies(X, drop_first=True, dtype=int)\n",
    "# Force float just to be safe\n",
    "X = X.astype(float)\n",
    "# Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "#Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_val)\n",
    "# FIX: Use np.sqrt() instead of squared=False\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_val, lr_preds))\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.4f}\")\n",
    "#Decision Tree \n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt_param_grid = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(dt, dt_param_grid, cv=5, scoring='neg_root_mean_squared_error')\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "dt_preds = dt_grid.predict(X_val)\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_val, dt_preds)) # FIX\n",
    "print(f\"Decision Tree RMSE:     {dt_rmse:.4f}\")\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20]\n",
    "}\n",
    "rf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='neg_root_mean_squared_error')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "rf_preds = rf_grid.predict(X_val)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_val, rf_preds))\n",
    "print(f\"Random Forest RMSE:     {rf_rmse:.4f}\")\n",
    "#Comparison: Default Random Forest vs Tuned Random Forest\n",
    "#Training a basic Random Forest with no hyperparameter tuning\n",
    "rf_default = RandomForestRegressor(random_state=42)\n",
    "rf_default.fit(X_train, y_train)\n",
    "rf_default_preds = rf_default.predict(X_val)\n",
    "rf_default_rmse = np.sqrt(mean_squared_error(y_val, rf_default_preds))\n",
    "\n",
    "print(f\"Random Forest (Default) RMSE: {rf_default_rmse:.4f}\")\n",
    "print(f\"Random Forest (Tuned)   RMSE: {rf_rmse:.4f}\")\n",
    "\n",
    "# Improvement Calculation\n",
    "improvement = rf_default_rmse - rf_rmse\n",
    "print(f\"Hyperparameter tuning improved RMSE by: {improvement:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b261f7",
   "metadata": {},
   "source": [
    "Random Forest RMSE:     51.8281\n",
    "Random Forest (Default) RMSE: 51.8279\n",
    "Random Forest (Tuned)   RMSE: 51.8281\n",
    "Hyperparameter tuning improved RMSE by: -0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc22fc2",
   "metadata": {},
   "source": [
    "Finally loading in predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "#Encode categorical variables\n",
    "X_test_submission = pd.get_dummies(test_df, drop_first=True, dtype=int)\n",
    "\n",
    "#Aligning columns with Training Data\n",
    "#adding missing columns (filled with 0) and removes extra cols\n",
    "X_test_submission = X_test_submission.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "#Generate Predictions using your Best Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "best_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'random_state': 42\n",
    "}\n",
    "rf_best = RandomForestRegressor(**best_params)\n",
    "rf_best.fit(X_train, y_train)\n",
    "final_predictions = rf_best.predict(X_test_submission)\n",
    "#Saving CSV \n",
    "submission = pd.DataFrame({\n",
    "    'Gig_Id': test_df.index, \n",
    "    'Crowd_Energy': final_predictions\n",
    "})\n",
    "submission.to_csv('sample_submission.csv', index=False)\n",
    "print(\"predictions.csv generated successfully!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80fb56b",
   "metadata": {},
   "source": [
    "revenue optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d191ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to store my results\n",
    "prices = []\n",
    "profits = []\n",
    "audience = []\n",
    "energies = []\n",
    "# checking prices from 50 to 200 with gap of 5\n",
    "for p in range(50, 201, 5):\n",
    "    # linear formula for audience\n",
    "    # if price is 0, audience is 800. if price is 200, audience is 0\n",
    "    a = 800 * (1 - (p/200))\n",
    "    a = int(a) # people can't be float\n",
    "    \n",
    "    if a < 0:\n",
    "        a = 0\n",
    "        \n",
    "     #making a temporary row to predict energy\n",
    "     #used the average values for other columns\n",
    "    row = {\n",
    "        'Venue_ID': 'V_Gamma',\n",
    "        'Ticket_Price_USD': p,\n",
    "        'Crowd_Size': a,\n",
    "        'Volume_Level': 4.5, # average volume\n",
    "        'Day_of_Week': 'Saturday', # assume weekend\n",
    "        'Weather': 'Clear',\n",
    "        'Show_DateTime': pd.to_datetime('2024-01-01 20:00:00')\n",
    "        #certain date time taken to avoid crashing of code\n",
    "    }\n",
    "    \n",
    "    #into dataframe\n",
    "    temp_df = pd.DataFrame([row])\n",
    "    #feature engineering for rows\n",
    "    temp_df['Price_per_Crowd'] = temp_df['Ticket_Price_USD'] / (temp_df['Crowd_Size'] + 1)\n",
    "    temp_df['Crowd_Density'] = temp_df['Crowd_Size'] / 800\n",
    "    temp_df['Time_Bucket'] = 'Evening'\n",
    "    temp_df['Is_Weekend'] = 1\n",
    "    temp_df['Log_Crowd_Size'] = np.log1p(temp_df['Crowd_Size'])\n",
    "    temp_df['Log_Ticket_Price'] = np.log1p(temp_df['Ticket_Price_USD'])\n",
    "    temp_df['Chaos_Index'] = temp_df['Volume_Level'] * temp_df['Crowd_Density']\n",
    "    \n",
    "    #calculating price shock manually\n",
    "    avg_price_gamma = df2[df2['Venue_ID'] == 'V_Gamma']['Ticket_Price_USD'].mean()\n",
    "    if p > avg_price_gamma:\n",
    "        temp_df['Price_Shock'] = 1\n",
    "    else:\n",
    "        temp_df['Price_Shock'] = 0\n",
    "        \n",
    "    #encoding\n",
    "    temp_df = pd.get_dummies(temp_df, drop_first=True, dtype=int)\n",
    "    \n",
    "    #fixing missing columns\n",
    "    #aligning with X_train columns from before\n",
    "    temp_df = temp_df.reindex(columns=X_train.columns, fill_value=0)\n",
    "    #predicting energy\n",
    "    energy = rf_best.predict(temp_df)[0]\n",
    "    #calculating money\n",
    "    #ticket sales\n",
    "    money_tickets = p * a\n",
    "    \n",
    "    #extra commodities(drinks/merch)\n",
    "    #assuming base spend is 20 and energy increases it\n",
    "    extra_spend = 20 * (1 + energy/200)\n",
    "    money_extra = a * extra_spend\n",
    "    #costs\n",
    "    cost = 5000 + (8 * a) #8 per head(given)\n",
    "    #total profit\n",
    "    total = money_tickets + money_extra - cost\n",
    "    #adding to list\n",
    "    prices.append(p)\n",
    "    profits.append(total)\n",
    "    audience.append(a)\n",
    "    energies.append(energy)\n",
    "\n",
    "#finding the max profit\n",
    "max_p = max(profits)\n",
    "index_best = profits.index(max_p)\n",
    "best_price = prices[index_best]\n",
    "print(\"Best Price is:\", best_price)\n",
    "print(\"Max Profit is:\", max_p)\n",
    "print(\"Audience at best price:\", audience[index_best])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483e8ae",
   "metadata": {},
   "source": [
    "Plotting curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the graph\n",
    "plt.plot(prices, profits)\n",
    "plt.title(\"Profit vs Price\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.axvline(x=best_price, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fa7f9",
   "metadata": {},
   "source": [
    "Thank you ☺️"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
